{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TinyLLM: A Hands-On Introduction to AI/ML and LLMs\n",
                "\n",
                "This Jupyter Notebook serves as a practical guide for students and enthusiasts to delve into the fundamentals of Artificial Intelligence (AI), Machine Learning (ML), Neural Networks (NN), and Large Language Models (LLMs) by building a smaller, more manageable model.\n",
                "\n",
                "Our focus is on creating a \"TinyLLM\" using the **TinyStories dataset**. This dataset is specifically designed to be small and simple, making it ideal for:\n",
                "*   **Learning Core Concepts:** Understand the end-to-end process of an LLM workflow, from data preparation to training and inference.\n",
                "*   **Hardware Accessibility:** Train a functional LLM even on consumer-grade hardware, overcoming common barriers for students.\n",
                "*   **Rapid Experimentation:** Quickly iterate and observe the effects of changes due to faster training times.\n",
                "\n",
                "By working through this notebook, you will gain hands-on experience with:\n",
                "*   Setting up a Python environment for deep learning.\n",
                "*   Loading and processing text data for LLM training.\n",
                "*   Understanding the architecture of a transformer-based language model.\n",
                "*   Training an LLM from scratch.\n",
                "*   Performing interactive text generation (inference).\n",
                "*   Evaluating the performance of your trained model.\n",
                "\n",
                "This project is structured to provide a clear, step-by-step learning path, allowing you to grasp complex concepts through practical application."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Prerequisites\n",
                "\n",
                "This section ensures that the notebook is running in the correct directory for all subsequent operations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "%cd ..\n",
                "print(f\"Current working directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies\n",
                "\n",
                "Before running the model, ensure all necessary Python packages are installed. This project uses `torch`, `transformers`, and `datasets`.\n",
                "\n",
                "Run the following cell to install dependencies from `requirements.txt`. If you are using a virtual environment, make sure it's activated before launching Jupyter, or specify the full path to your Python executable within the virtual environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.5. Debug Data Loading (Optional)\n",
                "\n",
                "This cell is for debugging the data loading process. It will help determine if the Hugging Face dataset is being loaded correctly or if the fallback text is being used."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ensure we are in the correct directory\n",
                "%pwd\n",
                "\n",
                "from src.llm_data import load_and_process_dataset\n",
                "\n",
                "print(\"\\nAttempting to load and process the dataset...\")\n",
                "try:\n",
                "    train_data, val_data = load_and_process_dataset()\n",
                "    print(f\"\\n--- Data Loading Summary ---\")\n",
                "    print(f\"Training data samples: {len(train_data)}\")\n",
                "    print(f\"Validation data samples: {len(val_data)}\")\n",
                "    print(\"---------------------------\")\n",
                "except Exception as e:\n",
                "    print(f\"\\nAn error occurred during data loading: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.6. Verify Label Alignment (Optional)\n",
                "\n",
                "This cell allows you to verify that the input and label tensors are correctly shifted for causal language modeling after the recent bug fix. The decoded label should be the decoded input shifted by one token."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.llm_data import get_batch, decode, load_and_process_dataset\n",
                "\n",
                "# Ensure data is loaded (run this if you haven't already in the notebook)\n",
                "train_data, val_data = load_and_process_dataset()\n",
                "\n",
                "# Get a batch of data\n",
                "x, y = get_batch('train', train_data, val_data)\n",
                "\n",
                "# Decode and print the input and label for the first sample in the batch\n",
                "print(\"Decoded input:\\n\", decode(x[0].tolist()))\n",
                "print(\"Decoded label:\\n\", decode(y[0].tolist()))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Train the TinyLLM Model\n",
                "\n",
                "This step trains the TinyLLM model using the `src/main_train.py` script. The model will be trained for `100000` iterations on the `TinyStories` dataset and saved to `models/tinystories_llm_v1.pth`.\n",
                "\n",
                "**Note**: This training process can can take a significant amount of time depending on your hardware (CPU/GPU/MPS)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import sys\n",
                "\n",
                "print(\"Starting TinyLLM model training with real-time output...\")\n",
                "\n",
                "command = [sys.executable, \"-m\", \"src.main_train\"]\n",
                "\n",
                "process = subprocess.Popen(\n",
                "    command,\n",
                "    stdout=subprocess.PIPE,\n",
                "    stderr=subprocess.STDOUT,\n",
                "    text=True,\n",
                "    bufsize=1,\n",
                "    encoding=\"utf-8\",      # ✅ Fix: use UTF-8 for decoding\n",
                "    errors=\"replace\"       # ✅ Optional: replaces un-decodable characters\n",
                ")\n",
                "\n",
                "# Stream output line-by-line\n",
                "for line in process.stdout:\n",
                "    print(line, end='')\n",
                "\n",
                "process.wait()\n",
                "\n",
                "if process.returncode == 0:\n",
                "    print(\"\\n✅ Training script finished successfully.\")\n",
                "else:\n",
                "    print(f\"\\n❌ Training script exited with error code {process.returncode}.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.5. Plot Training Metrics\n",
                "\n",
                "After training, visualize the training progress by plotting the metrics logged during the training process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.analytics.plot_metrics import plot_training_metrics\n",
                "from src.config import TRAINING_LOG_FILE_PATH\n",
                "\n",
                "print(f\"Plotting training metrics from: {TRAINING_LOG_FILE_PATH}\")\n",
                "try:\n",
                "    plot_training_metrics(TRAINING_LOG_FILE_PATH)\n",
                "    print(\"Training metrics plots generated successfully in src/analytics/logs/.\")\n",
                "except FileNotFoundError:\n",
                "    print(f\"Error: Training log file not found at {TRAINING_LOG_FILE_PATH}. Please ensure training was completed successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred during plotting: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run Interactive Inference\n",
                "\n",
                "After the model has been successfully trained and saved, you can use the `src/main_inference.py` script to interactively generate text.\n",
                "\n",
                "The inference script will load the `tinystories_llm_v1.pth` model and prompt you to enter text. The model will then attempt to complete your input. The generation uses a `temperature` of `0.8` for more varied output.\n",
                "\n",
                "**To exit the interactive session, type `exit` when prompted.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python -m src.main_inference"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluate the TinyLLM Model\n",
                "\n",
                "This section allows you to evaluate the trained TinyLLM model using the `src/main_eval.py` script. It includes options to test the Gemini API connection, calculate perplexity, generate sample stories, and perform automated LLM-as-a-Judge evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n--- Testing Gemini API Connection ---\")\n",
                "!python -m src.main_eval --test-connection\n",
                "\n",
                "print(\"\\n--- Running Full Evaluation ---\")\n",
                "!python -m src.main_eval --perplexity --samples --judge\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
