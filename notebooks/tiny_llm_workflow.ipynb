{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\tiny_llm\n",
      "Current working directory: d:\\Projects\\tiny_llm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%cd ..\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyLLM: A Hands-On Introduction to AI/ML and LLMs\n",
    "\n",
    "This Jupyter Notebook serves as a practical guide for students and enthusiasts to delve into the fundamentals of Artificial Intelligence (AI), Machine Learning (ML), Neural Networks (NN), and Large Language Models (LLMs) by building a smaller, more manageable model.\n",
    "\n",
    "Our focus is on creating a \"TinyLLM\" using the **TinyStories dataset**. This dataset is specifically designed to be small and simple, making it ideal for:\n",
    "*   **Learning Core Concepts:** Understand the end-to-end process of an LLM workflow, from data preparation to training and inference.\n",
    "*   **Hardware Accessibility:** Train a functional LLM even on consumer-grade hardware, overcoming common barriers for students.\n",
    "*   **Rapid Experimentation:** Quickly iterate and observe the effects of changes due to faster training times.\n",
    "\n",
    "By working through this notebook, you will gain hands-on experience with:\n",
    "*   Setting up a Python environment for deep learning.\n",
    "*   Loading and processing text data for LLM training.\n",
    "*   Understanding the architecture of a transformer-based language model.\n",
    "*   Training an LLM from scratch.\n",
    "*   Performing interactive text generation (inference).\n",
    "*   Evaluating the performance of your trained model.\n",
    "\n",
    "This project is structured to provide a clear, step-by-step learning path, allowing you to grasp complex concepts through practical application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Before running the model, ensure all necessary Python packages are installed. This project uses `torch`, `transformers`, and `datasets`.\n",
    "\n",
    "Run the following cell to install dependencies from `requirements.txt`. If you are using a virtual environment, make sure it's activated before launching Jupyter, or specify the full path to your Python executable within the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Debug Data Loading (Optional)\n",
    "\n",
    "This cell is for debugging the data loading process. It will help determine if the Hugging Face dataset is being loaded correctly or if the fallback text is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we are in the correct directory\n",
    "%pwd\n",
    "\n",
    "from src.llm_data import load_and_process_dataset\n",
    "\n",
    "print(\"\\nAttempting to load and process the dataset...\")\n",
    "try:\n",
    "    train_data, val_data = load_and_process_dataset()\n",
    "    print(f\"\\n--- Data Loading Summary ---\")\n",
    "    print(f\"Training data samples: {len(train_data)}\")\n",
    "    print(f\"Validation data samples: {len(val_data)}\")\n",
    "    print(\"---------------------------\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during data loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Verify Label Alignment (Optional)\n",
    "\n",
    "This cell allows you to verify that the input and label tensors are correctly shifted for causal language modeling after the recent bug fix. The decoded label should be the decoded input shifted by one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_data import get_batch, decode, load_and_process_dataset\n",
    "\n",
    "# Ensure data is loaded (run this if you haven't already in the notebook)\n",
    "train_data, val_data = load_and_process_dataset()\n",
    "\n",
    "# Get a batch of data\n",
    "x, y = get_batch('train', train_data, val_data)\n",
    "\n",
    "# Decode and print the input and label for the first sample in the batch\n",
    "print(\"Decoded input:\\n\", decode(x[0].tolist()))\n",
    "print(\"Decoded label:\\n\", decode(y[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the TinyLLM Model\n",
    "\n",
    "This step trains the TinyLLM model using the `src/main_train.py` script. The model will be trained for `100000` iterations on the `TinyStories` dataset and saved to `models/tinystories_llm_v1.pth`.\n",
    "\n",
    "**Note**: This training process can can take a significant amount of time depending on your hardware (CPU/GPU/MPS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TinyLLM model training with real-time output...\n",
      "Using CUDA (NVIDIA GPU) for training.\n",
      "\n",
      "Loading pre-trained tokenizer: gpt2...\n",
      "Set tokenizer pad_token to eos_token.\n",
      "Tokenizer loaded. Vocabulary Size: 50257\n",
      "Model initialized with 14,138,705 parameters.\n",
      "Current working directory: d:\\Projects\\tiny_llm\n",
      "Expected model save path: d:\\Projects\\tiny_llm\\models\\tinystories_llm_v1.pth\n",
      "Resumed from iteration 4001\n",
      "Loading and processing dataset from Hugging Face...\n",
      "Dataset tokenized.\n",
      "Dataset processed and grouped.\n",
      "Training data samples: 1672858\n",
      "Validation data samples: 185874\n",
      "\n",
      "Starting training on cuda...\n",
      "Step 5000: train loss 2.1785, val loss 2.1761, lr 0.000432\n",
      "  Time Spent: 0h 1m 42s\n",
      "  Estimated Total: 0h 29m 1s\n",
      "  Estimated Remaining: 0h 27m 19s\n",
      "  Resources: CPU: 25.8% | RAM: 36.3% | GPU: 83% | VRAM: 15.57/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 6000: train loss 2.1202, val loss 2.1221, lr 0.000403\n",
      "  Time Spent: 0h 3m 20s\n",
      "  Estimated Total: 0h 30m 50s\n",
      "  Estimated Remaining: 0h 27m 30s\n",
      "  Resources: CPU: 30.6% | RAM: 37.6% | GPU: 88% | VRAM: 15.46/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 7000: train loss 2.0774, val loss 2.0798, lr 0.000371\n",
      "  Time Spent: 0h 4m 59s\n",
      "  Estimated Total: 0h 31m 32s\n",
      "  Estimated Remaining: 0h 26m 33s\n",
      "  Resources: CPU: 27.5% | RAM: 38.8% | GPU: 82% | VRAM: 15.56/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 8000: train loss 2.0480, val loss 2.0471, lr 0.000335\n",
      "  Time Spent: 0h 6m 38s\n",
      "  Estimated Total: 0h 31m 53s\n",
      "  Estimated Remaining: 0h 25m 15s\n",
      "  Resources: CPU: 24.0% | RAM: 39.9% | GPU: 89% | VRAM: 15.56/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 9000: train loss 2.0131, val loss 2.0247, lr 0.000298\n",
      "  Time Spent: 0h 8m 17s\n",
      "  Estimated Total: 0h 32m 6s\n",
      "  Estimated Remaining: 0h 23m 49s\n",
      "  Resources: CPU: 25.1% | RAM: 40.8% | GPU: 88% | VRAM: 15.56/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 10000: train loss 1.9968, val loss 1.9969, lr 0.000259\n",
      "  Time Spent: 0h 9m 55s\n",
      "  Estimated Total: 0h 32m 14s\n",
      "  Estimated Remaining: 0h 22m 18s\n",
      "  Resources: CPU: 26.9% | RAM: 41.7% | GPU: 82% | VRAM: 15.56/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 11000: train loss 1.9785, val loss 1.9795, lr 0.000220\n",
      "  Time Spent: 0h 11m 34s\n",
      "  Estimated Total: 0h 32m 20s\n",
      "  Estimated Remaining: 0h 20m 45s\n",
      "  Resources: CPU: 24.7% | RAM: 42.4% | GPU: 83% | VRAM: 15.56/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 12000: train loss 1.9570, val loss 1.9551, lr 0.000182\n",
      "  Time Spent: 0h 13m 13s\n",
      "  Estimated Total: 0h 32m 25s\n",
      "  Estimated Remaining: 0h 19m 11s\n",
      "  Resources: CPU: 26.2% | RAM: 43.0% | GPU: 82% | VRAM: 15.56/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 13000: train loss 1.9458, val loss 1.9530, lr 0.000146\n",
      "  Time Spent: 0h 14m 52s\n",
      "  Estimated Total: 0h 32m 28s\n",
      "  Estimated Remaining: 0h 17m 35s\n",
      "  Resources: CPU: 27.2% | RAM: 43.6% | GPU: 88% | VRAM: 15.55/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 14000: train loss 1.9327, val loss 1.9354, lr 0.000113\n",
      "  Time Spent: 0h 16m 31s\n",
      "  Estimated Total: 0h 32m 32s\n",
      "  Estimated Remaining: 0h 16m 0s\n",
      "  Resources: CPU: 24.5% | RAM: 43.2% | GPU: 88% | VRAM: 15.55/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 15000: train loss 1.9257, val loss 1.9293, lr 0.000083\n",
      "  Time Spent: 0h 18m 10s\n",
      "  Estimated Total: 0h 32m 34s\n",
      "  Estimated Remaining: 0h 14m 23s\n",
      "  Resources: CPU: 26.6% | RAM: 43.8% | GPU: 83% | VRAM: 15.59/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 16000: train loss 1.9132, val loss 1.9157, lr 0.000058\n",
      "  Time Spent: 0h 19m 45s\n",
      "  Estimated Total: 0h 32m 31s\n",
      "  Estimated Remaining: 0h 12m 45s\n",
      "  Resources: CPU: 27.2% | RAM: 44.6% | GPU: 87% | VRAM: 15.43/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 17000: train loss 1.8991, val loss 1.9131, lr 0.000037\n",
      "  Time Spent: 0h 21m 22s\n",
      "  Estimated Total: 0h 32m 29s\n",
      "  Estimated Remaining: 0h 11m 6s\n",
      "  Resources: CPU: 26.7% | RAM: 45.0% | GPU: 85% | VRAM: 15.50/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 18000: train loss 1.9027, val loss 1.9041, lr 0.000022\n",
      "  Time Spent: 0h 22m 57s\n",
      "  Estimated Total: 0h 32m 27s\n",
      "  Estimated Remaining: 0h 9m 29s\n",
      "  Resources: CPU: 27.4% | RAM: 45.8% | GPU: 90% | VRAM: 15.45/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 19000: train loss 1.8990, val loss 1.9068, lr 0.000013\n",
      "  Time Spent: 0h 24m 32s\n",
      "  Estimated Total: 0h 32m 23s\n",
      "  Estimated Remaining: 0h 7m 50s\n",
      "  Resources: CPU: 23.2% | RAM: 45.1% | GPU: 84% | VRAM: 15.44/15.92 GB\n",
      "Saving checkpoint...\n",
      "Step 19999: train loss 1.8983, val loss 1.9012, lr 0.000010\n",
      "  Time Spent: 0h 26m 7s\n",
      "  Estimated Total: 0h 32m 20s\n",
      "  Estimated Remaining: 0h 6m 13s\n",
      "  Resources: CPU: 26.7% | RAM: 46.0% | GPU: 84% | VRAM: 15.58/15.92 GB\n",
      "Saving checkpoint...\n",
      "\n",
      "Training complete!\n",
      "\n",
      "Final training loss: 1.8983, validation loss: 1.9012\n",
      "\n",
      "Training script finished successfully.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Ensure we are in the correct directory\n",
    "%pwd\n",
    "\n",
    "print(\"Starting TinyLLM model training with real-time output...\")\n",
    "\n",
    "# Construct the command to run the training script\n",
    "# Use sys.executable to ensure the correct Python interpreter (e.g., from a venv) is used\n",
    "command = [sys.executable, \"-m\", \"src.main_train\"]\n",
    "\n",
    "# Execute the command using subprocess.Popen for real-time output\n",
    "# bufsize=1 ensures line-buffered output\n",
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)\n",
    "\n",
    "# Read and print output line by line in real-time\n",
    "for line in process.stdout:\n",
    "    print(line, end='') # end='' prevents extra newlines as print already includes them\n",
    "\n",
    "# Wait for the process to complete and get the return code\n",
    "process.wait()\n",
    "\n",
    "if process.returncode == 0:\n",
    "    print(\"\\nTraining script finished successfully.\")\n",
    "else:\n",
    "    print(f\"\\nTraining script exited with error code {process.returncode}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Interactive Inference\n",
    "\n",
    "After the model has been successfully trained and saved, you can use the `src/main_inference.py` script to interactively generate text.\n",
    "\n",
    "The inference script will load the `tinystories_llm_v1.pth` model and prompt you to enter text. The model will then attempt to complete your input. The generation uses a `temperature` of `0.8` for more varied output.\n",
    "\n",
    "**To exit the interactive session, type `exit` when prompted.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.main_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the TinyLLM Model\n",
    "\n",
    "This section allows you to evaluate the trained TinyLLM model using the `src/main_eval.py` script. It includes options to test the Gemini API connection, calculate perplexity, generate sample stories, and perform automated LLM-as-a-Judge evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Gemini API Connection ---\n",
      "Using CUDA (NVIDIA GPU) for training.\n",
      "\n",
      "--- Testing Gemini API Connection ---\n",
      "Successfully connected to Gemini API.\n",
      "\n",
      "--- Running Full Evaluation ---\n",
      "Using CUDA (NVIDIA GPU) for training.\n",
      "\n",
      "Loading pre-trained tokenizer: gpt2...\n",
      "Set tokenizer pad_token to eos_token.\n",
      "Tokenizer loaded. Vocabulary Size: 50257\n",
      "Loading model from checkpoint: models/tinystories_llm_v1.pth\n",
      "Loading and processing dataset from Hugging Face...\n",
      "Dataset tokenized.\n",
      "Dataset processed and grouped.\n",
      "Training data samples: 1672858\n",
      "Validation data samples: 185874\n",
      "\n",
      "Calculating perplexity...\n",
      "Perplexity on validation set: 6.65\n",
      "\n",
      "--- Generating Sample Stories ---\n",
      "\n",
      "Prompt: Once upon a time, in a land far away,\n",
      "Generated Text:\n",
      "Once upon a time, in a land far away, there was a big tree. The tree had many friends who liked to play and explore.\n",
      "\n",
      "One sunny day, the sun came up and went for an adventure. It saw many things like animals, trees, flowers, and even other birds. They all wanted to see what was on it. So, they decided to go home and look at everything.\n",
      "\n",
      "When they got back, they found some books on the ground. They were so happy that they made new friends with them. Then\n",
      "\n",
      "Prompt: The little robot woke up and saw\n",
      "Generated Text:\n",
      "The little robot woke up and saw the cat. The mouse was scared too, but it wanted to help. It flew away happily in its new home.\n",
      "\n",
      "When they got back home, the robot looked at itself and felt safe again. The little boy never forgot his hard work.Once upon a time, there was an elephant named Ellie. Ellie lived on a farm with her friends. One day, Ellie's friend Billy came over to play. They were playing hide-and-seek until they finally found Lily's\n",
      "\n",
      "Prompt: A curious cat followed a butterfly and found\n",
      "Generated Text:\n",
      "A curious cat followed a butterfly and found the right way back home. From that day on, he always listened to his mommy when she needed it.Once upon a time, there was a little girl named Lily. She loved to play outside with her friends. One day, they decided to make a big pot of water in the garden. They used some water to dig out some buckets together and put them into their bucket. \n",
      "\n",
      "After making the soup, Lily's friend came over to see what they were doing.\n",
      "\n",
      "--- Evaluating with LLM-as-a-Judge (Gemini) ---\n",
      "\n",
      "Raw LLM Judge Response:\n",
      "```json\n",
      "{\n",
      "  \"coherence_score\": 1,\n",
      "  \"creativity_score\": 3,\n",
      "  \"grammar_score\": 2,\n",
      "  \"justification\": \"The story is highly incoherent, with frequent, unexplained shifts in character (from a 'brave knight' to 'the little boy') and location (from a cave to 'his backyard'). Many sentences are grammatically incorrect or awkwardly phrased (e.g., 'It looked like nothing could do', 'With a smile and tried to escape'), making the narrative difficult to follow. While there are a few unique images, such as a 'big bear hopping around with a shiny rock', the overall disjointedness and logical inconsistencies significantly detract from any potential creativity or engagement.\"\n",
      "}\n",
      "```\n",
      "LLM Judge Evaluation:\n",
      "{\n",
      "  \"coherence_score\": 1,\n",
      "  \"creativity_score\": 3,\n",
      "  \"grammar_score\": 2,\n",
      "  \"justification\": \"The story is highly incoherent, with frequent, unexplained shifts in character (from a 'brave knight' to 'the little boy') and location (from a cave to 'his backyard'). Many sentences are grammatically incorrect or awkwardly phrased (e.g., 'It looked like nothing could do', 'With a smile and tried to escape'), making the narrative difficult to follow. While there are a few unique images, such as a 'big bear hopping around with a shiny rock', the overall disjointedness and logical inconsistencies significantly detract from any potential creativity or engagement.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Testing Gemini API Connection ---\")\n",
    "!python -m src.main_eval --test-connection\n",
    "\n",
    "print(\"\\n--- Running Full Evaluation ---\")\n",
    "!python -m src.main_eval --perplexity --samples --judge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
