# 5.0 Training the LLM: The `main_train.py` Script

Training a Large Language Model (LLM) is the process of teaching it to understand and generate human-like text by showing it vast amounts of data. `src/main_train.py` is the script that orchestrates this entire learning process for our `TinyLLM`. It brings together the model architecture (`model.py`), the data preparation (`data_utils.py`), and the configuration settings (`config.py`) to make the model learn.

## 5.1 The Goal of Training: Minimizing Loss

At its core, training is an optimization problem. The model makes predictions, we compare those predictions to the actual correct answers (the "targets"), and we calculate a "loss" (or error) value. The goal of training is to adjust the model's internal parameters (weights and biases) in such a way that this loss is minimized over time. A lower loss generally means the model is making better predictions.

## 5.2 Key Concepts in Training

*   **Forward Pass:** When input data is fed through the model to produce predictions.
*   **Loss Calculation:** Quantifying the difference between the model's predictions and the true targets.
*   **Backward Pass (Backpropagation):** Calculating the gradients of the loss with respect to every parameter in the model. These gradients tell us how much each parameter contributed to the error and in what direction it should be adjusted.
*   **Optimizer:** An algorithm (like AdamW in our case) that uses the gradients to update the model's parameters, taking small steps towards minimizing the loss.
*   **Epoch vs. Iteration:**
    *   **Iteration (or Step):** One single update of the model's parameters using one batch of data.
    *   **Epoch:** One full pass through the entire training dataset. Our `TinyLLM` uses `MAX_ITERS` (iterations) as its primary training duration metric.
*   **Training Data vs. Validation Data:**
    *   **Training Data:** The data the model actively learns from.
    *   **Validation Data:** A separate, unseen dataset used to monitor the model's performance during training. This helps detect **overfitting** (when the model learns the training data too well but performs poorly on new data).

## 5.3 Step-by-Step Breakdown of `main_train.py`

Let's walk through the `main` function in `src/main_train.py`.

```python
import torch
import os
import time
import psutil
import platform
import math # Import math for cosine scheduler

from .config import (
    DEVICE, LEARNING_RATE, MAX_ITERS, EVAL_INTERVAL, MODEL_PATH,
    WARMUP_ITERS, LR_DECAY_ITERS, MIN_LR, GRAD_CLIP # Import new config parameters
)
from .model import TinyLLM
from .llm_data import load_and_process_dataset, get_dynamic_batch, estimate_loss, init_tokenizer, VOCAB_SIZE
from .utils.llm_utils import get_resource_usage, format_time # Import moved utility functions

# Learning rate scheduler function
def get_lr(it):
    # 1) linear warmup for WARMUP_ITERS steps
    if it < WARMUP_ITERS:
        return LEARNING_RATE * it / WARMUP_ITERS
    # 2) if it > LR_DECAY_ITERS, return MIN_LR
    if it > LR_DECAY_ITERS:
        return MIN_LR
    # 3) in between, use cosine decay down to MIN_LR
    decay_ratio = (it - WARMUP_ITERS) / (LR_DECAY_ITERS - WARMUP_ITERS)
    assert 0 <= decay_ratio <= 1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 1.0 to 0.0
    return MIN_LR + coeff * (LEARNING_RATE - MIN_LR)

def main():
    global VOCAB_SIZE # Ensure VOCAB_SIZE is accessible

    # Initialize tokenizer and get VOCAB_SIZE
    _, VOCAB_SIZE = init_tokenizer()

    # Load and process dataset
    train_data, val_data = load_and_process_dataset()

    # Model Instantiation
    model = TinyLLM(VOCAB_SIZE)
    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Model initialized with {num_params:,} parameters.")
    model.to(DEVICE)

    print(f"Current working directory: {os.getcwd()}")
    print(f"Expected model save path: {os.path.abspath(MODEL_PATH)}")

    # Training Loop
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    print(f"\nStarting training on {DEVICE}...")
    start_time = time.time()
    
    # Variables for estimated time
    iter_times = []
    
    for iter in range(MAX_ITERS):
        # Determine and set the learning rate for the current iteration
        lr = get_lr(iter)
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        iter_start_time = time.time()

        if iter % EVAL_INTERVAL == 0 or iter == MAX_ITERS - 1:
            losses = estimate_loss(model, train_data, val_data)
            elapsed_time = time.time() - start_time
            
            # Calculate estimated time
            if iter > 0:
                avg_time_per_iter = sum(iter_times) / len(iter_times)
                estimated_total_time = avg_time_per_iter * MAX_ITERS
                estimated_remaining_time = estimated_total_time - elapsed_time
                
                print(f"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {lr:.6f}") # Added lr to print
                print(f"  Time Spent: {format_time(elapsed_time)}")
                print(f"  Estimated Total: {format_time(estimated_total_time)}")
                print(f"  Estimated Remaining: {format_time(estimated_remaining_time)}")
            else:
                print(f"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {lr:.6f}") # Added lr to print
                print(f"  Time Spent: {elapsed_time:.2f}s")

            # Resource usage
            print(f"  Resources: {get_resource_usage()}")

        xb, yb = get_dynamic_batch('train', train_data, val_data)
        logits, loss = model(xb, yb)
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP) # Apply gradient clipping
        optimizer.step()
        
        iter_times.append(time.time() - iter_start_time)

    print("\nTraining complete!")
    try:
        # Ensure the directory for the model path exists
        os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
        torch.save(model.state_dict(), MODEL_PATH)
        print(f"Model saved to {MODEL_PATH}")
    except Exception as e:
        print(f"Error saving model to {MODEL_PATH}: {e}")

if __name__ == '__main__':
    main()
```

### 5.3.1 Initialization and Setup

1.  **`global VOCAB_SIZE`**: Ensures we can access and potentially modify the global `VOCAB_SIZE` variable.
2.  **`_, VOCAB_SIZE = init_tokenizer()`**: Calls the `init_tokenizer` function from `data_utils.py` to load the tokenizer and determine the vocabulary size. This is crucial for initializing the model correctly.
3.  **`train_data, val_data = load_and_process_dataset()`**: Loads and preprocesses the dataset using the function from `data_utils.py`. This prepares our text data into numerical tensors ready for the model.
4.  **`model = TinyLLM(VOCAB_SIZE)`**: Instantiates our `TinyLLM` model (defined in `model.py`), passing the `vocab_size` so it knows the size of its output layer.
5.  **`num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)`**: Calculates and prints the total number of trainable parameters in the model. This gives an idea of the model's complexity.
6.  **`model.to(DEVICE)`**: Moves the model to the specified computing device (CPU, GPU, or MPS) for efficient computation.
7.  **`optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)`**:
    *   **Optimizer:** This line initializes the **optimizer**. `AdamW` is a popular and effective optimization algorithm.
    *   **`model.parameters()`**: Tells the optimizer which parameters (weights and biases) it needs to update.
    *   **`lr=LEARNING_RATE`**: Sets the learning rate, which controls the step size for parameter updates (as defined in `config.py`).

### 5.3.2 The Training Loop (`for iter in range(MAX_ITERS):`)

This is the core of the training process, where the model repeatedly learns from data.

1.  **Evaluation Check (`if iter % EVAL_INTERVAL == 0 or iter == MAX_ITERS - 1:`):**
    *   Periodically, the script pauses training to evaluate the model's performance on both the training and validation datasets. This happens every `EVAL_INTERVAL` iterations and at the very end of training.
    *   **`losses = estimate_loss(model, train_data, val_data)`**: Calls the `estimate_loss` function from `data_utils.py`. This function temporarily switches the model to evaluation mode, calculates the average loss on a few batches of both training and validation data, and then switches the model back to training mode.
    *   The training and validation losses are printed, along with the elapsed time, to monitor progress. A decreasing loss indicates that the model is learning. If the training loss continues to decrease but the validation loss starts to increase, it's a sign of **overfitting**.

2.  **Get Batch (`xb, yb = get_dynamic_batch('train', train_data, val_data)`):**
    *   Fetches a new batch of training data (`xb` for input, `yb` for target) from our `data_utils.py` module.

3.  **Forward Pass (`logits, loss = model(xb, yb)`):**
    *   The input batch `xb` is fed into the `model`.
    *   The `model.forward()` method (from `model.py`) computes the `logits` (raw predictions for the next token) and the `loss` (how far off the predictions are from the actual `yb` targets).

4.  **Backward Pass (Backpropagation):**
    *   **`optimizer.zero_grad(set_to_none=True)`**: Before calculating new gradients, it's crucial to clear any gradients from the previous iteration. If you don't do this, gradients would accumulate, leading to incorrect updates. `set_to_none=True` is often more memory-efficient.
    *   **`loss.backward()`**: This is the magic of PyTorch's automatic differentiation! It performs the **backward pass** (backpropagation). Based on the calculated `loss`, PyTorch automatically computes the gradient of the loss with respect to every trainable parameter in the model. These gradients indicate the direction and magnitude by which each parameter should be adjusted to reduce the loss.

5.  **Optimizer Step (`optimizer.step()`):**
    *   The `optimizer` uses the gradients computed in the backward pass to update the model's parameters (weights and biases). This is the actual "learning" step where the model adjusts itself to make better predictions in the future.

### 5.3.3 Saving the Model

*   **`os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)`**: Ensures that the directory where the model will be saved (`models/` in our case) actually exists. If it doesn't, it creates it.
*   **`torch.save(model.state_dict(), MODEL_PATH)`**: Saves the trained model's parameters (its "state dictionary") to the specified `MODEL_PATH`. This allows us to load the trained model later for inference or further evaluation without retraining.
*   **Error Handling:** A `try-except` block is used to catch any errors during the saving process, providing a helpful message if something goes wrong.

## 5.4 Conclusion

`src/main_train.py` is the central control script for teaching our `TinyLLM`. It orchestrates the iterative process of feeding data, making predictions, calculating errors, and adjusting the model's internal knowledge. By understanding this training loop, you grasp how LLMs acquire their impressive abilities to understand and generate language.
