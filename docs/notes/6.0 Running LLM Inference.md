# 6.0 Running LLM Inference: The `main_inference.py` Script

Once a Large Language Model (LLM) has been trained, it's ready to be put to work! The process of using a trained model to make predictions or generate new content is called **inference**. `src/main_inference.py` is the script dedicated to this task for our `TinyLLM`. It allows you to interact with the trained model by providing a prompt and having the model complete your text.

## 6.1 What is Inference?

Inference is essentially applying the "knowledge" the model gained during training to new, unseen inputs. For a language model, this means:
*   **Input:** You give it a starting piece of text (a "prompt").
*   **Prediction:** The model uses its learned patterns to predict the most probable next token.
*   **Generation:** This predicted token is then added to the input, and the process repeats, allowing the model to generate a longer, coherent sequence of text.

Unlike training, inference does not involve updating the model's parameters. It's a read-only operation on the model's learned state.

## 6.2 Step-by-Step Breakdown of `main_inference.py`

Let's examine the `main` function in `src/main_inference.py`.

```python
import torch
import os

from .config import DEVICE, MODEL_PATH
from .model import TinyLLM # Keep TinyLLM for type hinting if needed, but loading is now in utils
from .llm_data import init_tokenizer, encode, decode, VOCAB_SIZE
from .utils.llm_utils import get_model_size_mb, load_model_and_tokenizer, generate_text_from_model

# ASCII Art Logo for TinyLLM
TINYLLM_LOGO = r"""
  _____ _             _     _     __  __ 
 |_   _(_)_ __  _   _| |   | |   |  \/  |
   | | | | '_ \| | | | |   | |   | |\/| |
   | | | | | | | |_| | |___| |___| |  | |
   |_| |_|_| |_|\__, |_____|_____|_|  |_|
                |___/                    
"""

def display_model_card(model, model_path, vocab_size, device):
    """Displays a model card with key details and an example."""
    print("\n" + "="*50)
    print(TINYLLM_LOGO)
    print("                     TinyLLM Model Card")
    print("="*50)
    print("\nAbout:")
    print("  TinyStoriesLLM is a compact language model designed for generating short, coherent stories.")
    print("  It's trained on a small dataset to demonstrate fundamental LLM capabilities.")
    print(f"\nModel Details:")
    print(f"  Vocabulary Size: {vocab_size:,}")
    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  Parameters: {num_params:,}")
    model_size_mb = get_model_size_mb(model_path)
    print(f"  Model File Size: {model_size_mb:.2f} MB")
    print(f"  Device: {device}")

    print("\nExpected Input/Output Example:")
    example_prompt = "Once upon a time, there was a little"
    print(f"  Input Prompt: \"{example_prompt}\"")
    
    generated_text = generate_text_from_model(model, example_prompt) # Use default generation parameters
    if generated_text:
        # Find the first occurrence of the prompt in the generated text and slice from there
        start_index = generated_text.find(example_prompt)
        if start_index != -1:
            generated_text_display = generated_text[start_index:]
        else:
            generated_text_display = generated_text
        print(f"  Generated Output: \"{generated_text_display}\"")
    else:
        print("  (Error generating example.)")
    print("="*50 + "\n")


def main():
    global VOCAB_SIZE # Ensure VOCAB_SIZE is accessible

    model, VOCAB_SIZE = load_model_and_tokenizer()
    if model is None:
        print("Failed to load model. Exiting inference.")
        return

    print(f"Current working directory: {os.getcwd()}")
    print(f"Expected model load path: {os.path.abspath(MODEL_PATH)}")
    
    # Display model card after successful load
    display_model_card(model, MODEL_PATH, VOCAB_SIZE, DEVICE)

    # Interactive Inference
    print("\n--- Interactive Text Generation ---")
    print("Enter a starting prompt (or type 'exit' to quit).")
    print("The model will try to complete your text.")

    while True:
        prompt = input("Prompt: ")
        if prompt.lower() == 'exit':
            break

        if not prompt:
            print("Please enter a non-empty prompt.")
            continue

        full_generated_text = generate_text_from_model(model, prompt) # Use default generation parameters
        if full_generated_text:
            print("Generated:", full_generated_text)
        else:
            print("Could not generate text for the given prompt.")

    print("Exiting text generation.")

if __name__ == '__main__':
    main()
```

### 6.2.1 Initialization and Model Loading

1.  **`global VOCAB_SIZE`**: Ensures access to the global `VOCAB_SIZE`.
2.  **`_, VOCAB_SIZE = init_tokenizer()`**: Loads the tokenizer and gets the vocabulary size, just like in training. This is essential because the model's input and output layers are sized according to the vocabulary.
3.  **`model = TinyLLM(VOCAB_SIZE)`**: Instantiates the `TinyLLM` model.
4.  **`model.to(DEVICE)`**: Moves the model to the appropriate computing device (CPU, GPU, or MPS).
5.  **`if os.path.exists(MODEL_PATH): ...`**: This block checks if a trained model file (`.pth` file) exists at the `MODEL_PATH` specified in `config.py`.
    *   **`model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))`**: If the model file exists, this line loads the previously saved weights and biases into our `TinyLLM` instance. `map_location=DEVICE` ensures the model is loaded onto the correct device, regardless of which device it was trained on.
    *   **`model.eval()`**: This is a very important step for inference! It switches the model to "evaluation mode." In this mode, certain layers like Dropout (which randomly "switches off" neurons during training) are disabled, and Layer Normalization behaves consistently. This ensures deterministic and correct behavior during inference.
    *   **Error Handling:** If the model file is not found or there's an error loading it, the script prints a helpful message and exits, reminding the user to train the model first.

### 6.2.2 Interactive Text Generation Loop

After successfully loading the model, the script enters an infinite loop, allowing for interactive text generation.

1.  **`prompt = input("Prompt: ")`**: Prompts the user to enter a starting text.
2.  **`if prompt.lower() == 'exit': break`**: Allows the user to quit the program by typing "exit".
3.  **Input Validation:** Checks if the prompt is empty.
4.  **`context_ids = encode(prompt)`**: The user's text `prompt` is converted into numerical token IDs using the `encode` function from `data_utils.py`.
5.  **`context = torch.tensor(context_ids, dtype=torch.long, device=DEVICE).unsqueeze(0)`**:
    *   The list of token IDs is converted into a PyTorch tensor.
    *   `dtype=torch.long`: Specifies the data type for token IDs.
    *   `device=DEVICE`: Moves the tensor to the correct computing device.
    *   `.unsqueeze(0)`: Adds an extra dimension at the beginning. This is because models typically expect inputs in batches, even if it's a batch of one. So, `(sequence_length)` becomes `(1, sequence_length)`.
6.  **`generated_tokens = model.generate(context, max_new_tokens=200, temperature=0.8)[0].tolist()`**:
    *   This is the core generation call! The `model.generate()` method (defined in `model.py`) is called with:
        *   `context`: Our starting prompt (as a tensor).
        *   `max_new_tokens=200`: The maximum number of new tokens the model should generate.
        *   `temperature=0.8`: Controls the randomness of the generated text. A value of 0.8 makes the output slightly less deterministic than 1.0, encouraging some creativity without being too chaotic. (Refer to `4.0 Building the LLM Architecture` for more on temperature).
    *   `[0].tolist()`: The `generate` method returns a batch of generated sequences. We take the first (and only) sequence in the batch and convert its tokens back into a Python list.
7.  **`full_generated_text = decode(generated_tokens)`**: The list of generated token IDs is converted back into human-readable text using the `decode` function from `data_utils.py`.
8.  **`print("Generated:", full_generated_text)`**: The complete generated text is printed to the console.
9.  **Error Handling (`try...except` block):** Catches potential errors during encoding or generation, such as characters not being in the tokenizer's vocabulary.

## 6.3 Conclusion

`src/main_inference.py` provides a practical way to interact with your trained `TinyLLM`. It demonstrates the crucial steps of loading a saved model, setting it to evaluation mode, and then using its `generate` method to produce new text based on a given prompt. This script is your window into seeing the "intelligence" your LLM has acquired.
