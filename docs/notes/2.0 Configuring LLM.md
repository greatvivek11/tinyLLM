# 2.0 Configuring Your Large Language Model (LLM)

In any complex software project, especially in machine learning, it's crucial to have a centralized place to manage all the settings and parameters. This is exactly what `src/config.py` does for our `TinyLLM` project. Think of it as the control panel where you can tweak various knobs and dials that influence how your LLM behaves, trains, and performs.

## 2.1 Why Configuration is Important

Imagine building a house. You wouldn't just start laying bricks without a blueprint, right? Similarly, for an LLM:
*   **Reproducibility:** If you want to get the same results later, or share your work with others, having clear configurations ensures everyone is using the same settings.
*   **Experimentation:** You might want to try different settings (e.g., a larger model, a different learning rate) to see what works best. A config file makes it easy to change these values without digging through the entire codebase.
*   **Readability:** It keeps your main code files clean and focused on the logic, while parameters are neatly organized elsewhere.

## 2.2 Key Configuration Parameters in `config.py`

Let's break down the important parameters defined in `src/config.py` and understand what each one means in the context of our LLM.

### 2.2.1 Model Architecture Parameters

These parameters define the "size" and "shape" of our Transformer model.

*   **`BLOCK_SIZE = 256`**:
    *   **Concept:** This is also known as the **context window** or **sequence length**. It defines the maximum number of tokens (words or sub-word units) that the model can look at simultaneously when making a prediction.
    *   **Analogy:** If you're reading a book, `BLOCK_SIZE` is like the number of words you can hold in your immediate memory to understand the current word. If a sentence is longer than this, the model can only "see" the most recent `BLOCK_SIZE` words.
    *   **Impact:** A larger `BLOCK_SIZE` allows the model to capture longer-range dependencies in text (e.g., how a word at the beginning of a paragraph relates to a word at the end). However, it also significantly increases memory usage and computation time during training and inference.

*   **`D_MODEL = 256`**:
    *   **Concept:** This is the **embedding dimension** or **model dimension**. It's the size of the vector used to represent each token and each position in the sequence.
    *   **Analogy:** Imagine each word is represented by a point in a multi-dimensional space. `D_MODEL` is the number of dimensions in that space. A higher dimension allows for richer, more nuanced representations of words and their meanings.
    *   **Impact:** A larger `D_MODEL` generally leads to a more powerful model that can learn more complex patterns, but it also increases the number of parameters and computational cost.

*   **`NUM_HEADS = 8`**:
    *   **Concept:** This relates to **Multi-Head Self-Attention** (as discussed in `1.0 AI Fundamentals`). It's the number of independent "attention mechanisms" that run in parallel within each Transformer block.
    *   **Analogy:** If `D_MODEL` is the total number of dimensions for a word's representation, `NUM_HEADS` divides this into smaller chunks (`D_MODEL / NUM_HEADS`) for each attention head. Each head can then focus on different aspects of the input.
    *   **Impact:** More heads allow the model to capture a wider variety of relationships and dependencies within the input sequence, leading to better understanding.

*   **`NUM_LAYERS = 8`**:
    *   **Concept:** This is the number of **Transformer blocks** stacked on top of each other. Each Transformer block processes the input and passes its output to the next block.
    *   **Analogy:** Think of it as the depth of your model. Each layer refines the understanding of the input text.
    *   **Impact:** More layers allow the model to learn more abstract and hierarchical representations of the data, which is crucial for complex language tasks. However, deeper networks are harder to train and require more computation.

*   **`DROPOUT = 0.1`**:
    *   **Concept:** **Dropout** is a regularization technique used to prevent **overfitting**. Overfitting happens when a model learns the training data too well, including its noise and specific quirks, and then performs poorly on new, unseen data.
    *   **How it works:** During training, dropout randomly "switches off" a certain percentage (`DROPOUT` value) of neurons in a layer. This forces the network to learn more robust features, as it cannot rely on any single neuron or set of neurons.
    *   **Impact:** It helps the model generalize better to new data, making it more robust.

### 2.2.2 Training Parameters

These parameters control the process of teaching the LLM to learn from data.

*   **`LEARNING_RATE = 1e-4`**:
    *   **Concept:** This is one of the most critical hyperparameters for training. It determines the **step size** at which the model's weights and biases are adjusted during optimization.
    *   **Analogy:** Imagine you're trying to find the lowest point in a valley (which represents the lowest error). The learning rate is how big a step you take each time you move.
    *   **Impact:**
        *   A **high learning rate** might cause the model to overshoot the optimal solution, leading to unstable training or even divergence (the loss increases instead of decreases).
        *   A **low learning rate** might make training very slow, as the model takes tiny steps, and it might get stuck in a local minimum (a good but not the best solution).
        *   `1e-4` (which is 0.0001) is a common starting point for Transformer models.

*   **`MAX_ITERS = 10000`**:
    *   **Concept:** This is the total number of **training iterations** or **steps**. In each iteration, the model processes a batch of data, calculates the loss, and updates its parameters.
    *   **Impact:** More iterations generally mean more learning, but beyond a certain point, the model might start overfitting or the improvements might become negligible.

*   **`BATCH_SIZE = 32`**:
    *   **Concept:** During training, instead of feeding one example at a time, we group multiple examples into a **batch**. `BATCH_SIZE` is the number of examples processed in one training iteration.
    *   **Analogy:** If you're studying for an exam, `BATCH_SIZE` is like the number of practice questions you do before checking your answers and adjusting your study method.
    *   **Impact:**
        *   **Larger batches** provide a more stable estimate of the gradient (the direction to adjust weights), but require more memory.
        *   **Smaller batches** introduce more noise into the gradient estimate, which can sometimes help escape local minima, but training can be less stable.

*   **`EVAL_INTERVAL = 500`**:
    *   **Concept:** During training, it's important to periodically check how well the model is performing on data it hasn't seen before (validation data). `EVAL_INTERVAL` specifies how often (in terms of training iterations) this evaluation should happen.
    *   **Impact:** Frequent evaluation gives you more insight into the training progress, but it adds overhead.

*   **`EVAL_ITERS = 200`**:
    *   **Concept:** When we evaluate the model (at `EVAL_INTERVAL`), we don't want to evaluate on the entire validation dataset, as that can be slow. `EVAL_ITERS` specifies how many batches of validation data to use for a quick estimate of the validation loss.
    *   **Impact:** A higher `EVAL_ITERS` gives a more accurate estimate of the validation loss but takes longer.

### 2.2.3 Learning Rate Scheduler and Gradient Clipping Parameters

These parameters control the advanced aspects of the training process, specifically how the learning rate changes over time and how gradients are managed.

*   **`WARMUP_ITERS = 100`**:
    *   **Concept:** During the initial phase of training, it's often beneficial to gradually increase the learning rate from a very small value to the `LEARNING_RATE`. This is called **warmup**. `WARMUP_ITERS` defines the number of iterations over which this linear warmup occurs.
    *   **Impact:** Warmup helps stabilize training at the beginning, especially with large models and high learning rates, preventing large gradient updates that could destabilize the network.

*   **`LR_DECAY_ITERS = MAX_ITERS`**:
    *   **Concept:** After the warmup phase, the learning rate typically decays over time. This parameter defines the total number of iterations over which the learning rate will decay from `LEARNING_RATE` down to `MIN_LR`. Setting it to `MAX_ITERS` means the decay happens throughout the entire training process after warmup.
    *   **Impact:** Learning rate decay helps the model fine-tune its parameters more precisely as training progresses, leading to better convergence and performance.

*   **`MIN_LR = 1e-5`**:
    *   **Concept:** This is the minimum learning rate that the scheduler will decay to. The learning rate will not go below this value, even if `LR_DECAY_ITERS` is reached.
    *   **Impact:** Prevents the learning rate from becoming too small, which could halt learning prematurely.

*   **`GRAD_CLIP = 1.0`**:
    *   **Concept:** **Gradient clipping** is a technique used to prevent **exploding gradients**, a problem where gradients become excessively large during backpropagation, leading to unstable training and divergence.
    *   **How it works:** If the norm (magnitude) of the gradients exceeds a certain threshold (`GRAD_CLIP`), the gradients are scaled down proportionally.
    *   **Impact:** Stabilizes training, especially in deep networks or with large learning rates, by ensuring that gradient updates do not become too large.

### 2.2.4 Evaluation Parameters

These parameters control how the model's performance is evaluated.

*   **`EVAL_BATCHES_PERPLEXITY = 100`**:
    *   **Concept:** This is similar to `EVAL_ITERS` but specifically for calculating **perplexity**, a common metric for language models. It defines how many batches of validation data are used for this specific calculation.
    *   **Perplexity:** Perplexity is a measure of how well a probability model predicts a sample. In simpler terms, a lower perplexity means the model is better at predicting the next word in a sequence, indicating a better language model.

### 2.2.5 Generation Parameters

These parameters influence how the model generates new text during inference.

*   **`MAX_NEW_TOKENS = 100`**:
    *   **Concept:** The maximum number of new tokens the model will generate in a single inference call.
    *   **Impact:** Controls the length of the generated text.

*   **`TEMPERATURE = 0.7`**:
    *   **Concept:** A parameter used during text generation to control the randomness of the output.
    *   **Impact:**
        *   **High temperature (e.g., > 1.0):** Makes the probability distribution flatter, increasing the likelihood of sampling less probable (and thus more "creative" or "random") tokens.
        *   **Low temperature (e.g., < 1.0):** Makes the probability distribution sharper, increasing the likelihood of sampling the most probable tokens, leading to more deterministic and conservative output.

*   **`TOP_K = 50`**:
    *   **Concept:** **Top-k sampling** is a technique to control randomness during generation. Instead of considering all possible next tokens, it only considers the `top_k` most probable tokens and redistributes their probabilities.
    *   **Impact:** Prevents the model from generating very unlikely (and often nonsensical) tokens, leading to more coherent output.

*   **`TOP_P = 0.95`**:
    *   **Concept:** **Top-p sampling** (also known as nucleus sampling) is another advanced sampling technique. It selects the smallest set of tokens whose cumulative probability exceeds a threshold `p`.
    *   **Impact:** Offers a more dynamic way to control the vocabulary size for sampling compared to `top_k`, adapting to the shape of the probability distribution.

*   **`REPETITION_PENALTY = 1.2`**:
    *   **Concept:** A factor by which the logits of already generated tokens are reduced.
    *   **Impact:** Discourages the model from repeating words or phrases, leading to more diverse and less repetitive generated text.

### 2.2.6 Device Configuration

This section determines which hardware (CPU, GPU, or Apple's MPS) the model will use for computations.

*   **`DEVICE = "mps"`, `"cuda"`, or `"cpu"`**:
    *   **Concept:** Machine learning models, especially large ones, require significant computational power. Modern computers have specialized hardware for this:
        *   **CPU (Central Processing Unit):** The general-purpose processor in your computer. It's versatile but slower for parallel computations.
        *   **GPU (Graphics Processing Unit):** Originally designed for rendering graphics, GPUs are excellent at performing many calculations in parallel, making them ideal for neural network training. NVIDIA's GPUs use **CUDA** technology.
        *   **MPS (Metal Performance Shaders):** Apple's framework for high-performance computing on its silicon (M1, M2, etc.) chips. It's optimized for Apple hardware.
    *   **Logic:** The code checks for the availability of these devices in a specific order: MPS first (for Apple users), then CUDA (for NVIDIA GPU users), and finally defaults to CPU if neither is found.
    *   **Impact:** Using a GPU or MPS significantly speeds up training and inference compared to a CPU.

### 2.2.4 Data Preparation and Tokenizer Configuration

These parameters relate to how the text data is handled and converted into a format the model can understand.

*   **`FALLBACK_TEXT`**:
    *   **Concept:** This is a hardcoded string of text used as a backup dataset.
    *   **Purpose:** If the primary method of loading data (e.g., from a Hugging Face dataset) fails due to network issues or other errors, the model can still train on this small, predefined text. This ensures the training script can always run, even if external resources are unavailable.

*   **`TOKENIZER_MODEL_NAME = 'gpt2'`**:
    *   **Concept:** A **tokenizer** is a crucial component in NLP. It breaks down raw text into smaller units called **tokens** (which can be words, parts of words, or even characters) and converts them into numerical IDs that the model can process.
    *   **Pre-trained Tokenizer:** `'gpt2'` refers to the pre-trained tokenizer from the GPT-2 model. Using a pre-trained tokenizer is beneficial because it already knows how to handle common words and sub-word units, which helps the model understand language better.
    *   **Impact:** The choice of tokenizer affects how the text is represented numerically, which directly impacts the model's ability to learn from and generate text.

*   **`VOCAB_SIZE = None`**:
    *   **Concept:** The **vocabulary size** is the total number of unique tokens that the tokenizer knows.
    *   **Dynamic Setting:** It's set to `None` initially because its actual value will be determined dynamically when the `bert-base-uncased` tokenizer is loaded. Each tokenizer has a fixed vocabulary.

### 2.2.5 Model Path

*   **`MODEL_PATH = 'models/tinystories_llm_v1.pth'`**:
    *   **Concept:** This specifies the file path where the trained model's "knowledge" (its learned weights and biases) will be saved after training, and from where it will be loaded for inference or evaluation.
    *   **`.pth` extension:** This is a common extension for PyTorch model files.

## 2.3 Conclusion

The `config.py` file acts as the central hub for all these critical settings. By understanding each parameter, you gain insight into the fundamental choices made when designing, training, and deploying an LLM. Modifying these values allows for experimentation and optimization, which are key parts of machine learning development.
