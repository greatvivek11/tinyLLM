# 7.0 Evaluating the LLM: The `main_eval.py` Script

Training a Large Language Model (LLM) is only half the battle; the other half is knowing how well it performs! `src/main_eval.py` is designed to assess the quality of our `TinyLLM` using various metrics and techniques. This script helps us understand if our model is learning effectively, generating coherent text, and meeting our expectations.

## 7.1 Why Evaluation is Crucial

*   **Measure Progress:** During training, evaluation metrics tell us if the model is improving.
*   **Detect Issues:** It helps identify problems like overfitting (when the model memorizes training data but fails on new data) or underfitting (when the model hasn't learned enough).
*   **Compare Models:** Allows us to compare different models or different training configurations to find the best one.
*   **Understand Capabilities:** Provides insights into what the model is good at and where it struggles.

## 7.2 Key Evaluation Metrics and Techniques

`main_eval.py` focuses on three main ways to evaluate our LLM:

1.  **Perplexity:** A standard quantitative metric for language models.
2.  **Sample Generation:** Qualitative assessment by generating and reviewing text.
3.  **LLM-as-a-Judge:** Using another, more powerful LLM to provide an objective (or semi-objective) quality assessment.

## 7.3 Step-by-Step Breakdown of `main_eval.py`

Let's explore the `main` function and its helper functions in `src/main_eval.py`.

```python
import torch
import os
import json
import argparse
from .config import DEVICE, MODEL_PATH, BLOCK_SIZE, EVAL_BATCHES_PERPLEXITY
from .model import TinyLLM
from .data_utils import init_tokenizer, decode, load_and_process_dataset, get_dynamic_batch
from .utils.llm_utils import test_gemini_connection, get_gemini_model, load_model_and_tokenizer, generate_text_from_model

@torch.no_grad()
def calculate_perplexity(model, val_data):
    """Calculates perplexity on the validation set."""
    print("\nCalculating perplexity...")
    model.eval()
    total_loss = 0
    total_tokens = 0

    # Evaluate on a subset of batches for faster calculation
    for k in range(EVAL_BATCHES_PERPLEXITY):
        X, Y = get_dynamic_batch('val', None, val_data) # get_dynamic_batch expects train_data as first arg, but we only need val_data here
        _, loss = model(X, Y)
        total_loss += loss.item() * X.size(1) # Accumulate loss weighted by sequence length
        total_tokens += X.size(1) # Accumulate total tokens processed

    avg_loss = total_loss / total_tokens
    perplexity = torch.exp(torch.tensor(avg_loss))
    print(f"Perplexity on validation set: {perplexity.item():.2f}")
    return perplexity.item()

def generate_samples(model, num_samples=3):
    """Generates sample stories from a set of prompts."""
    print("\n--- Generating Sample Stories ---")
    prompts = [
        "Once upon a time, in a land far away,",
        "The little robot woke up and saw",
        "A curious cat followed a butterfly and found"
    ]
    
    for i in range(min(num_samples, len(prompts))):
        prompt_text = prompts[i]
        print(f"\nPrompt: {prompt_text}")
        
        generated_text = generate_text_from_model(model, prompt_text) # Use default generation parameters
        
        print("Generated Text:")
        print(generated_text)

def evaluate_with_llm_judge(story):
    """Uses an external LLM (Gemini) to judge the quality of a generated story."""
    print("\n--- Evaluating with LLM-as-a-Judge (Gemini) ---")
    
    model = get_gemini_model()
    if model is None:
        print("Skipping LLM-as-a-Judge because Gemini model could not be initialized.")
        return None

    judge_prompt = f"""
    You are an expert in evaluating children's stories. Please rate the following story on a scale of 1 to 10 for the given criteria.
    Provide your response in a JSON format.

    Story:
    "{story}"

    Criteria:
    1. Coherence: Does the story make logical sense?
    2. Creativity: Is the story original and engaging?
    3. Grammar: Is the story grammatically correct?

    JSON Response Format:
    {{
      "coherence_score": <score>,
      "creativity_score": <score>,
      "grammar_score": <score>,
      "justification": "<brief justification for your scores>"
    }}
    """

    try:
        response = model.generate_content(judge_prompt)
        
        # Print the raw response text for debugging
        print("\nRaw LLM Judge Response:")
        print(response.text)

        # Extract JSON from markdown code block if present
        json_string = response.text.strip()
        if json_string.startswith("```json") and json_string.endswith("```"):
            json_string = json_string[len("```json"): -len("```")].strip()
        
        # Attempt to parse the JSON string
        result = json.loads(json_string) 
        print("LLM Judge Evaluation:")
        print(json.dumps(result, indent=2))
        return result
    except json.JSONDecodeError as e:
        print(f"JSON decoding error from LLM-as-a-Judge: {e}")
        print("The LLM judge did not return valid JSON. Check the raw response above.")
        return None
    except Exception as e:
        print(f"An unexpected error occurred during LLM-as-a-Judge evaluation with Gemini: {e}")
        return None

def main():
    parser = argparse.ArgumentParser(description="Evaluation script for TinyLLM.")
    parser.add_argument('--perplexity', action='store_true', help="Run perplexity calculation.")
    parser.add_argument('--samples', action='store_true', help="Run sample generation.")
    parser.add_argument('--judge', action='store_true', help="Run LLM-as-a-Judge evaluation.")
    parser.add_argument('--test-connection', action='store_true', help="Test Gemini API connection.")
    args = parser.parse_args()

    run_all = not (args.perplexity or args.samples or args.judge or args.test_connection)

    if args.test_connection:
        test_gemini_connection()
        return # Exit after testing connection

    model, vocab_size = load_model_and_tokenizer()
    if model is None:
        return

    if run_all or args.perplexity:
        _, val_data = load_and_process_dataset()
        calculate_perplexity(model, val_data)

    if run_all or args.samples:
        generate_samples(model)

    if run_all or args.judge:
        prompt = "A brave knight went on an adventure to"
        story = generate_text_from_model(model, prompt, max_new_tokens=150) # Use default temperature, top_p, repetition_penalty
        evaluate_with_llm_judge(story)

if __name__ == '__main__':
    main()
```

### 7.3.1 Command-Line Arguments (`argparse`)

The script uses `argparse` to allow users to specify which evaluation tasks to run from the command line.
*   `--perplexity`: Run perplexity calculation.
*   `--samples`: Generate sample stories.
*   `--judge`: Run LLM-as-a-Judge evaluation.
*   `--test-connection`: Test connection to the Gemini API (used for LLM-as-a-Judge).
*   If no arguments are provided, all evaluation tasks are run by default.

### 7.3.2 Loading Model and Tokenizer (`load_model_and_tokenizer`)

*   This function is similar to the one in `main_inference.py`.
*   It initializes the tokenizer, instantiates the `TinyLLM` model, and attempts to load the trained model's `state_dict` from `MODEL_PATH`.
*   Crucially, it sets `model.eval()` to ensure the model is in evaluation mode, which is necessary for consistent and correct evaluation results.
*   It includes error handling if the model file is not found or cannot be loaded.

### 7.3.3 Calculating Perplexity (`calculate_perplexity`)

*   **`@torch.no_grad()` and `model.eval()`**: As discussed in `3.0 Loading and Processing Data`, these ensure that gradients are not computed and the model is in evaluation mode.
*   **Perplexity Calculation:**
    *   The function iterates `EVAL_BATCHES_PERPLEXITY` times, fetching validation data batches.
    *   For each batch, it performs a forward pass (`model(X, Y)`) to get the `loss`.
    *   **`total_loss += loss.item() * X.size(1)`**: The loss is accumulated, weighted by the sequence length (`X.size(1)`). This is because `CrossEntropyLoss` averages over tokens, so we need to multiply by the number of tokens to get the total loss for the batch.
    *   **`total_tokens += X.size(1)`**: Accumulates the total number of tokens processed.
    *   **`avg_loss = total_loss / total_tokens`**: Calculates the average loss per token.
    *   **`perplexity = torch.exp(torch.tensor(avg_loss))`**: Perplexity is mathematically defined as `e^(average_loss)`. A lower perplexity indicates a better language model, meaning it's more confident and accurate in predicting the next word.
*   The calculated perplexity is printed.

### 7.3.4 Generating Samples (`generate_samples`)

*   **Purpose:** This provides a qualitative way to assess the model's generation capabilities.
*   It defines a few fixed `prompts` (e.g., "Once upon a time,...").
*   For each prompt:
    *   The prompt is encoded into token IDs and converted to a tensor.
    *   **`model.generate(context, max_new_tokens=100, temperature=0.7)`**: The model's `generate` method (from `model.py`) is called to produce new text. A `temperature` of 0.7 is used to make the generation somewhat creative but still coherent.
    *   The generated tokens are decoded back into human-readable text and printed. This allows a human to read and judge the fluency, coherence, and creativity of the generated stories.

### 7.3.5 Evaluating with LLM-as-a-Judge (`evaluate_with_llm_judge`)

*   **Concept:** This is an advanced technique where a more powerful, external LLM (like Google's Gemini model) is used to evaluate the output of our `TinyLLM`. This can provide a more objective and automated way to assess qualitative aspects like coherence, creativity, and grammar, which are hard to measure with simple numerical metrics.
*   **`get_gemini_model()`**: This function (from `llm_utils.py`) attempts to initialize the Gemini model. If the API key is not set or there's a connection issue, this evaluation step will be skipped.
*   **`judge_prompt`**: A carefully crafted prompt is given to the Gemini model. This prompt instructs Gemini to act as an "expert in evaluating children's stories" and to rate the provided story based on specific criteria (coherence, creativity, grammar) and return the results in a JSON format. This structured output is crucial for programmatic parsing.
*   **`response = model.generate_content(judge_prompt)`**: The generated story from `TinyLLM` is embedded within the `judge_prompt`, and this combined prompt is sent to the Gemini model.
*   **JSON Parsing:** The response from Gemini is expected to be in JSON format. The code attempts to parse this JSON, including handling cases where Gemini might wrap the JSON in a markdown code block (` ```json ... ``` `).
*   The parsed evaluation scores and justification from the Gemini model are printed.
*   **Error Handling:** Includes robust error handling for JSON decoding issues or other problems with the Gemini API call.

### 7.3.6 Main Execution Logic (`main`)

*   **`parser = argparse.ArgumentParser(...)`**: Sets up the command-line argument parser.
*   **`run_all = not (...)`**: Determines if all evaluation tasks should run (if no specific arguments are provided).
*   **Conditional Execution:** Based on the command-line arguments or `run_all`, the script calls the relevant evaluation functions: `test_gemini_connection`, `calculate_perplexity`, `generate_samples`, and `evaluate_with_llm_judge`.

## 7.4 Conclusion

`src/main_eval.py` is essential for understanding the performance and capabilities of your trained `TinyLLM`. By combining quantitative metrics like perplexity with qualitative assessments through sample generation and the innovative LLM-as-a-Judge approach, you get a comprehensive view of your model's strengths and weaknesses, guiding future improvements.
