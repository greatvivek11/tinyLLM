# 3.0 Loading and Processing Data for Your LLM

Before a Large Language Model (LLM) can learn anything, it needs data! And not just any data, but data that's been carefully prepared and formatted in a way the model can understand. This is the crucial role of `src/llm_data.py`. This file handles everything from fetching raw text to transforming it into numerical sequences that our `TinyLLM` can process.

## 3.1 The Data Pipeline: From Text to Tensors

Think of the data pipeline as a factory assembly line:
1.  **Raw Material (Text):** We start with human-readable text (like stories or articles).
2.  **Tokenization:** This text is broken down into smaller units called "tokens" (words, parts of words, punctuation).
3.  **Numerical Conversion:** Each token is converted into a unique numerical ID.
4.  **Batching:** These numerical sequences are grouped into "batches" for efficient processing by the model.
5.  **Tensor Conversion:** Finally, these batches are converted into `torch.Tensor` objects, which are the fundamental data structures used by PyTorch (our deep learning framework).

`src/llm_data.py` orchestrates these steps.

## 3.2 Key Components and Functions in `llm_data.py`

Let's explore the functions and concepts within `src/llm_data.py`.

### 3.2.1 Initializing the Tokenizer (`init_tokenizer`)

```python
def init_tokenizer():
    global tokenizer, VOCAB_SIZE
    if tokenizer is None:
        print(f"\nLoading pre-trained tokenizer: {TOKENIZER_MODEL_NAME}...")
        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL_NAME)
        VOCAB_SIZE = tokenizer.vocab_size
        print(f"Tokenizer loaded. Vocabulary Size: {VOCAB_SIZE}")
    return tokenizer, VOCAB_SIZE
```
*   **Purpose:** This function is responsible for loading our pre-trained tokenizer.
*   **`global tokenizer, VOCAB_SIZE`**: These lines indicate that the function will modify the global `tokenizer` and `VOCAB_SIZE` variables. This is a common pattern to ensure the tokenizer is loaded only once and is accessible throughout the module.
*   **`AutoTokenizer.from_pretrained(TOKENIZER_MODEL_NAME)`**: This is a powerful feature from the Hugging Face `transformers` library. It automatically downloads and loads the tokenizer associated with the specified model name (e.g., `'gpt2'`). It also includes logic to set `tokenizer.pad_token` to `tokenizer.eos_token` if `pad_token` is initially `None`, ensuring consistent padding behavior. Using a pre-trained tokenizer is crucial because it ensures that the numerical IDs for words are consistent with what the model expects, and it handles complex linguistic nuances like sub-word tokenization.
*   **`tokenizer.vocab_size`**: After loading, we extract the `vocab_size`, which is the total number of unique tokens the tokenizer knows. This value is essential for our `TinyLLM` model to know how many possible words it can predict.

### 3.2.2 Encoding and Decoding Text (`encode`, `decode`)

```python
def encode(s):
    global tokenizer
    if tokenizer is None:
        init_tokenizer()
    return tokenizer.encode(s, add_special_tokens=False)

def decode(l):
    global tokenizer
    if tokenizer is None:
        init_tokenizer()
    return tokenizer.decode(l, skip_special_tokens=True)
```
*   **`encode(s)`**:
    *   **Purpose:** Converts a raw text string (`s`) into a list of numerical token IDs.
    *   **`add_special_tokens=False`**: For our language modeling task, we usually don't need special tokens (like `[CLS]` for classification or `[SEP]` for separating sentences) at the beginning/end of the input sequence, so we set this to `False`.
*   **`decode(l)`**:
    *   **Purpose:** Converts a list of numerical token IDs (`l`) back into a human-readable text string. This is essential for understanding the model's generated output.
    *   **`skip_special_tokens=True`**: Ensures that any special tokens (if they were somehow introduced) are not included in the decoded text, making the output cleaner.

### 3.2.3 Getting a Batch of Data (`get_batch`)

```python
def get_batch(split, train_data, val_data):
    data = train_data if split == 'train' else val_data
    
    if isinstance(data, torch.Tensor): # Handle fallback text (raw tensor)
        # ... (safeguard and slicing logic for raw tensor)
        ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))
        x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])
        y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])
    else: # Handle Hugging Face Dataset (dictionary-like)
        ix = torch.randint(len(data), (BATCH_SIZE,))
        batch = [data[i.item()] for i in ix]
        x = torch.stack([item['input_ids'] for item in batch])
        y = torch.stack([item['labels'] for item in batch])
    
    x, y = x.to(DEVICE), y.to(DEVICE)
    return x, y
```
*   **Purpose:** This function fetches a "batch" of data for training or validation.
*   **`split`**: Determines whether to get data from the training set (`'train'`) or validation set (`'val'`).
*   **`train_data`, `val_data`**: The actual datasets.
*   **Batching Logic:**
    *   **Safeguard:** Includes a check (`if len(data) < BLOCK_SIZE + 1:`) to ensure the data is long enough to form a full batch, repeating the data if necessary to prevent errors.
    *   **`torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))`**: This generates `BATCH_SIZE` random starting indices (`ix`). Each index `i` represents the beginning of a sequence.
    *   **`x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])`**: For each random index `i`, it takes a sequence of `BLOCK_SIZE` tokens starting from `i`. These are our input sequences (`x`).
    *   **`y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])`**: For each input sequence `x`, the corresponding target sequence `y` is created by shifting `x` by one position. This is the core of **language modeling**: the model is trained to predict the *next* token given the current sequence. So, if `x` is `[token1, token2, token3]`, `y` is `[token2, token3, token4]`.
*   **`x.to(DEVICE), y.to(DEVICE)`**: This moves the data to the specified computing device (CPU, GPU, or MPS) for faster processing.

### 3.2.4 Estimating Loss (`estimate_loss`)

```python
@torch.no_grad()
def estimate_loss(model, train_data, val_data):
    out = {}
    model.eval() # Set model to evaluation mode
    for split in ['train', 'val']:
        losses = torch.zeros(EVAL_ITERS)
        for k in range(EVAL_ITERS):
            X, Y = get_batch(split, train_data, val_data)
            logits, loss = model(X, Y) # Forward pass
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train() # Set model back to training mode
    return out
```
*   **`@torch.no_grad()`**: This decorator is crucial! It tells PyTorch not to calculate gradients during this function's execution. We only need gradients during training (to update weights), not during evaluation. This saves memory and speeds up computation.
*   **`model.eval()`**: Puts the model in "evaluation mode." This is important because certain layers (like Dropout and Layer Normalization) behave differently during training and evaluation. For example, Dropout is active during training but turned off during evaluation.
*   **Looping for Evaluation:** The function iterates `EVAL_ITERS` times, fetching a batch, performing a forward pass (`model(X, Y)`), and recording the loss.
*   **`loss.item()`**: Extracts the numerical value of the loss from the PyTorch tensor.
*   **`losses.mean()`**: Calculates the average loss over all evaluation batches for both training and validation splits.
*   **`model.train()`**: After evaluation, the model is switched back to "training mode" so that subsequent training steps behave correctly.

### 3.2.5 Loading and Processing the Dataset (`load_and_process_dataset`)

```python
def load_and_process_dataset():
    global VOCAB_SIZE
    init_tokenizer() # Initialize tokenizer and set VOCAB_SIZE

    try:
        print("Loading and processing dataset from Hugging Face...")
        dataset = load_dataset('roneneldan/TinyStories', split='train')

        def tokenize_function(examples):
            local_tokenizer, _ = init_tokenizer()
            return local_tokenizer(examples["text"], add_special_tokens=True) # Now uses add_special_tokens=True

        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            num_proc=8, # Uses 8 processes for parallel tokenization (note: on Windows, this may effectively run with 1 process due to multiprocessing limitations)
            remove_columns=["text"]
        )
        print("Dataset tokenized.")

        def group_texts(examples):
            concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
            total_length = len(concatenated_examples[list(examples.keys())[0]])
            total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE
            result = {
                k: [t[i : i + BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)]
                for k, t in concatenated_examples.items()
            }
            result["labels"] = result["input_ids"].copy()
            return result

        processed_dataset = tokenized_dataset.map(
            group_texts,
            batched=True,
            num_proc=8, # Uses 8 processes for parallel tokenization (note: on Windows, this may effectively run with 1 process due to multiprocessing limitations)
        )
        print("Dataset processed and grouped.")

        processed_dataset.set_format(type='torch', columns=['input_ids', 'labels'])
        
        split_dataset = processed_dataset.train_test_split(test_size=0.1)
        train_data = split_dataset['train']
        val_data = split_dataset['test']
        
        print(f"Training data samples: {len(train_data)}")
        print(f"Validation data samples: {len(val_data)}")
        return train_data, val_data

    except Exception as e:
        print(f"Error processing dataset: {e}")
        print("Falling back to hardcoded text.")
        text = FALLBACK_TEXT
        data = torch.tensor(encode(text), dtype=torch.long)
        n = int(0.9 * len(data))
        train_data = data[:n]
        val_data = data[n:]
        return train_data, val_data
```
*   **Purpose:** This is the main function for getting our training and validation data. It attempts to load a dataset from Hugging Face and, if that fails, falls back to a hardcoded text.
*   **`load_dataset('roneneldan/TinyStories', split='train')`**: This line uses the Hugging Face `datasets` library to download the "TinyStories" dataset. This dataset is specifically designed for training small language models.
*   **`tokenize_function`**: This nested function is applied to the dataset to tokenize the raw text.
    *   **`dataset.map(tokenize_function, ...)`**: The `map` function applies `tokenize_function` to every example in the dataset.
    *   **`batched=True`**: Processes multiple examples at once, which is more efficient.
    *   **`num_proc=8`**: Attempts to use 8 processes for parallel tokenization. (Note: The comment indicates it's set to 1 for Windows compatibility, which is a good practice for avoiding multiprocessing issues).
    *   **`remove_columns=["text"]`**: After tokenization, the original "text" column is no longer needed.
*   **`group_texts`**: This function takes the tokenized sequences and groups them into fixed-size blocks (`BLOCK_SIZE`).
    *   **Concatenation:** It first concatenates all tokenized sequences into one long sequence.
    *   **Chunking:** Then, it splits this long sequence into chunks of `BLOCK_SIZE`. This is important because our Transformer model expects fixed-size inputs.
    *   **`result["labels"] = result["input_ids"].copy()`**: For language modeling, the target (`labels`) for a given input sequence is simply the input sequence itself, shifted by one token. This is because the model predicts the *next* token.
*   **`processed_dataset.set_format(type='torch', ...)`**: Converts the dataset format to PyTorch tensors, making it compatible with our model.
*   **`processed_dataset.train_test_split(test_size=0.1)`**: Splits the processed dataset into a training set (90%) and a validation/test set (10%). This is crucial for evaluating how well our model generalizes to unseen data.
*   **Error Handling (`try...except` block):** If downloading or processing the Hugging Face dataset fails (e.g., no internet connection), the code gracefully falls back to using the `FALLBACK_TEXT` defined in `config.py`. This ensures the script can always run.

## 3.3 Conclusion

The `llm_data.py` file is the unsung hero of our LLM project. It takes messy, raw text and transforms it into the perfectly structured numerical data that our `TinyLLM` needs to learn. Understanding this pipeline is fundamental to comprehending how any language model consumes and processes information.
