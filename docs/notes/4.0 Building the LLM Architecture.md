# 4.0 Building the LLM Architecture: The `model.py` Deep Dive

This is where the magic happens! `src/model.py` is the heart of our `TinyLLM` project. It defines the actual structure of our Large Language Model, implementing the Transformer architecture we discussed in `1.0 AI Fundamentals`. Understanding this file is key to grasping how an LLM processes information and generates text.

We'll break down the model into its core components, starting from the smallest building blocks and assembling them into the complete `TinyLLM`.

## 4.1 Core Concepts in `model.py`

Before diving into the code, let's recap some essential concepts from `1.0 AI Fundamentals` that are directly implemented here:
*   **Multi-Head Self-Attention:** The mechanism that allows the model to weigh the importance of different parts of the input sequence.
*   **Feed-Forward Networks:** Simple neural networks that process information independently for each token.
*   **Residual Connections:** Adding the input of a layer to its output to help with training deep networks.
*   **Layer Normalization:** Normalizing activations to stabilize and speed up training.
*   **Token Embeddings:** Converting numerical token IDs into dense vector representations.
*   **Positional Embeddings:** Adding information about the position of tokens in a sequence.

## 4.2 The `MultiHeadSelfAttention` Module

This class implements the crucial self-attention mechanism.

```python
class MultiHeadSelfAttention(nn.Module):
    """
    Multiple heads of self-attention in parallel.
    This module allows the model to attend to different parts of the input sequence
    simultaneously, capturing various relationships.
    """
    def __init__(self, d_model, num_heads, dropout):
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        self.d_k = d_model // num_heads # Dimension of each head's key/query/value
        self.num_heads = num_heads

        # Linear layers for Key, Query, Value projections for all heads
        self.wq = nn.Linear(d_model, d_model)
        self.wk = nn.Linear(d_model, d_model)
        self.wv = nn.Linear(d_model, d_model)

        # Output linear layer after concatenating head outputs
        self.wo = nn.Linear(d_model, d_model)
        self.attn_dropout = nn.Dropout(dropout)
        self.resid_dropout = nn.Dropout(dropout)

        # Causal mask: ensures that tokens only attend to previous tokens.
        # This is crucial for language modeling to prevent "peeking" at future tokens.
        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE))
                                     .view(1, 1, BLOCK_SIZE, BLOCK_SIZE))

    def forward(self, x):
        B, T, C = x.shape # Batch, Time (sequence length), Channels (d_model)

        # 1. Project inputs to Q, K, V for all heads
        # Shape: (B, T, C) -> (B, T, num_heads, d_k) -> (B, num_heads, T, d_k)
        q = self.wq(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)
        k = self.wk(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)
        v = self.wv(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)

        # 2. Compute attention scores (QK^T)
        # (B, num_heads, T, d_k) @ (B, num_heads, d_k, T) -> (B, num_heads, T, T)
        attn_scores = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_k))

        # 3. Apply causal mask
        # Set attention scores to a very small negative number for future tokens,
        # so they become 0 after softmax.
        attn_scores = attn_scores.masked_fill(self.tril[:,:,:T,:T] == 0, float('-inf'))

        # 4. Apply softmax to get attention weights
        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_weights = self.attn_dropout(attn_weights)

        # 5. Multiply weights by Values (AV)
        # (B, num_heads, T, T) @ (B, num_heads, T, d_k) -> (B, num_heads, T, d_k)
        out = attn_weights @ v

        # 6. Concatenate heads and apply final linear layer
        # (B, num_heads, T, d_k) -> (B, T, num_heads * d_k) -> (B, T, d_model)
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        out = self.resid_dropout(self.wo(out))
        return out
```

### 4.2.1 `__init__` (Initialization)

*   **`d_model`, `num_heads`, `dropout`**: These are configuration parameters from `config.py`.
    *   `d_model`: The overall dimension of the model (e.g., 128).
    *   `num_heads`: How many parallel attention mechanisms (e.g., 4).
    *   `dropout`: The dropout rate for regularization (e.g., 0.1).
*   **`self.d_k = d_model // num_heads`**: This calculates the dimension of the Query, Key, and Value vectors for *each individual attention head*. If `d_model` is 128 and `num_heads` is 4, then `d_k` is 32. Each head will process a 32-dimensional slice of the overall `d_model` representation.
*   **`self.wq`, `self.wk`, `self.wv`**: These are `nn.Linear` layers. They are responsible for projecting the input `x` into the Query (Q), Key (K), and Value (V) vectors. Crucially, these layers transform the input `d_model` dimension into `d_model` dimension, which is then split across `num_heads`.
*   **`self.wo`**: This is the output linear layer. After all the attention heads have processed their information and their outputs are concatenated, this layer combines them back into the original `d_model` dimension.
*   **`self.attn_dropout`, `self.resid_dropout`**: These apply dropout to the attention weights and the final output, respectively, to prevent overfitting.
*   **`self.register_buffer('tril', ...)`**: This creates the **causal mask**.
    *   **`torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE))`**: Creates a lower triangular matrix of ones. For example, if `BLOCK_SIZE` is 4:
        ```
        [[1., 0., 0., 0.],
         [1., 1., 0., 0.],
         [1., 1., 1., 0.],
         [1., 1., 1., 1.]]
        ```
    *   **`view(1, 1, BLOCK_SIZE, BLOCK_SIZE)`**: Reshapes it to be compatible for broadcasting with attention scores.
    *   **Why Causal Mask?** In language generation, a model should only predict the *next* word based on the *previous* words. It should not "peek" at future words. The causal mask ensures this by setting the attention scores for future tokens to negative infinity, so they become zero after the softmax operation. This makes the attention "causal" or "masked."

### 4.2.2 `forward` (Computation)

This method defines how data flows through the attention mechanism.

1.  **`B, T, C = x.shape`**: Extracts batch size (`B`), sequence length (`T`, which is `BLOCK_SIZE`), and channel dimension (`C`, which is `D_MODEL`) from the input `x`.
2.  **`q = self.wq(x).view(...)`, `k = self.wk(x).view(...)`, `v = self.wv(x).view(...)`**:
    *   The input `x` is passed through the Query, Key, and Value linear layers.
    *   `.view(B, T, self.num_heads, self.d_k)`: Reshapes the output to separate the heads.
    *   `.transpose(1, 2)`: Swaps the `T` (sequence length) and `num_heads` dimensions. This arranges the tensor so that each head's data is contiguous, making subsequent matrix multiplications easier. The shape becomes `(B, num_heads, T, d_k)`.
3.  **`attn_scores = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_k))`**:
    *   **`q @ k.transpose(-2, -1)`**: This is the core of attention: Query multiplied by the transpose of Key. This calculates the "similarity" or "relevance" between each query token and all key tokens. The result is a matrix of attention scores for each head.
    *   **`* (1.0 / math.sqrt(self.d_k))`**: This is a scaling factor. It's important to scale the attention scores to prevent the dot products from becoming too large, which can push the softmax function into regions where it has very small gradients, hindering learning.
4.  **`attn_scores = attn_scores.masked_fill(self.tril[:,:,:T,:T] == 0, float('-inf'))`**: Applies the causal mask. Where the `tril` mask is 0 (meaning it's a future token), the attention score is set to negative infinity.
5.  **`attn_weights = F.softmax(attn_scores, dim=-1)`**: Applies the softmax function along the last dimension. This converts the attention scores into probabilities (weights) that sum to 1 for each token, indicating how much attention each token should pay to other tokens.
6.  **`out = attn_weights @ v`**: The attention weights are multiplied by the Value vectors. This is where the information from relevant tokens is aggregated. Each output token is a weighted sum of the Value vectors of all input tokens.
7.  **`out = out.transpose(1, 2).contiguous().view(B, T, C)`**:
    *   `.transpose(1, 2)`: Reverts the transpose operation from step 1.
    *   `.contiguous()`: Ensures the tensor is stored contiguously in memory, which is often required before a `view` operation.
    *   `.view(B, T, C)`: Reshapes the output back to the original `(Batch, Sequence Length, D_MODEL)` shape, effectively concatenating the outputs from all attention heads.
8.  **`out = self.resid_dropout(self.wo(out))`**: The concatenated output is passed through the final linear layer (`self.wo`) and then dropout is applied. This is the output of the Multi-Head Self-Attention module.

## 4.3 The `FeedForward` Module

This is a simpler, position-wise feed-forward network.

```python
class FeedForward(nn.Module):
    """
    A simple two-layer feed-forward network with a ReLU activation.
    This processes information independently for each token position.
    """
    def __init__(self, d_model, dropout):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_model, 4 * d_model), # Expansion layer
            nn.ReLU(),
            nn.Linear(4 * d_model, d_model), # Projection back to d_model
            nn.Dropout(dropout),
        )

    def forward(self, x):
        return self.net(x)
```
*   **Purpose:** This network processes each token's representation independently. It's applied to each position in the sequence separately and identically.
*   **`nn.Linear(d_model, 4 * d_model)`**: An "expansion" layer that projects the `d_model` input into a larger dimension (4 times `d_model`).
*   **`nn.ReLU()`**: The Rectified Linear Unit activation function, which introduces non-linearity.
*   **`nn.Linear(4 * d_model, d_model)`**: A "projection" layer that maps the expanded dimension back to the original `d_model`.
*   **`nn.Dropout(dropout)`**: Applies dropout for regularization.
*   **`nn.Sequential`**: A convenient container that allows you to stack modules in a sequential order.

## 4.4 The `TransformerBlock` Module

This module combines the attention and feed-forward networks, along with residual connections and layer normalization, to form a complete Transformer block.

```python
class TransformerBlock(nn.Module):
    """
    A single Transformer block, combining Multi-Head Self-Attention and a Feed-Forward network.
    Includes Layer Normalization and Residual Connections (Add & Norm).
    """
    def __init__(self, d_model, num_heads, dropout):
        super().__init__()
        self.attn = MultiHeadSelfAttention(d_model, num_heads, dropout)
        self.ffwd = FeedForward(d_model, dropout)
        self.ln1 = nn.LayerNorm(d_model) # Normalizes across the feature dimension
        self.ln2 = nn.LayerNorm(d_model)

    def forward(self, x):
        # Residual connection 1: x + Attention(LayerNorm(x))
        x = x + self.attn(self.ln1(x))
        # Residual connection 2: x + FeedForward(LayerNorm(x))
        x = x + self.ffwd(self.ln2(x))
        return x
```
*   **Purpose:** This is the fundamental repeating unit of the Transformer model.
*   **`self.attn`, `self.ffwd`**: Instances of our `MultiHeadSelfAttention` and `FeedForward` modules.
*   **`self.ln1`, `self.ln2`**: `nn.LayerNorm` layers.
    *   **Layer Normalization:** Unlike Batch Normalization (which normalizes across the batch dimension), Layer Normalization normalizes across the feature dimension for each individual sample. This helps stabilize training, especially in recurrent or transformer networks.
*   **`x = x + self.attn(self.ln1(x))`**: This implements the first **residual connection** and **Layer Normalization**.
    *   The input `x` is first normalized (`self.ln1(x)`).
    *   Then, it's passed through the attention mechanism (`self.attn(...)`).
    *   Finally, the original input `x` is *added* to the output of the attention mechanism. This "skip connection" allows gradients to flow more easily through the network, preventing vanishing gradients and enabling the training of very deep models.
*   **`x = x + self.ffwd(self.ln2(x))`**: This implements the second residual connection and Layer Normalization, similar to the first, but for the feed-forward network.

## 4.5 The `TinyLLM` (Main Model)

This is the complete `TinyLLM` model, assembling all the pieces.

```python
class TinyLLM(nn.Module):
    """
    The complete Tiny Language Model.
    It comprises token embeddings, positional embeddings,
    a stack of Transformer blocks, and a final linear layer (LM head).
    """
    def __init__(self, vocab_size): # Add vocab_size as a parameter
        super().__init__()
        # Token embeddings: maps input token IDs to dense vectors.
        self.token_embedding_table = nn.Embedding(vocab_size, D_MODEL) # Use vocab_size parameter
        # Positional embeddings: adds information about the token's position in the sequence.
        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, D_MODEL)
        # Stack of Transformer blocks.
        self.blocks = nn.Sequential(*[
            TransformerBlock(D_MODEL, NUM_HEADS, DROPOUT) for _ in range(NUM_LAYERS)
        ])
        self.ln_f = nn.LayerNorm(D_MODEL) # Final layer norm
        # Language Model head: projects the output of the Transformer blocks
        # back to the vocabulary size to get logits for each possible next token.
        self.lm_head = nn.Linear(D_MODEL, vocab_size) # Use vocab_size parameter

        # Initialize weights (optional, but good practice)
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        B, T = idx.shape # Batch size, Sequence length

        # Get token and positional embeddings
        tok_emb = self.token_embedding_table(idx) # (B, T, D_MODEL)
        pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE)) # (T, D_MODEL)
        x = tok_emb + pos_emb # (B, T, D_MODEL) - Sum token and positional embeddings

        # Pass through Transformer blocks
        x = self.blocks(x) # (B, T, D_MODEL)
        x = self.ln_f(x) # (B, T, D_MODEL)

        # Project to vocabulary size to get logits
        logits = self.lm_head(x) # (B, T, VOCAB_SIZE)

        loss = None
        if targets is not None:
            # Reshape logits and targets for CrossEntropyLoss
            # PyTorch expects (N, C, ...) for input and (N, ...) for target
            B, T, C = logits.shape
            logits = logits.view(B * T, C)
            targets = targets.view(B * T)
            loss = F.cross_entropy(logits, targets) # Calculate loss

        return logits, loss

    @torch.no_grad() # Disable gradient calculation for inference.
    def generate(self, idx, max_new_tokens, temperature=0.8, top_k=50, top_p=0.95, repetition_penalty=1.2):
        """
        Generates new text based on an initial sequence (idx).
        The model predicts the next token, adds it to the sequence, and repeats.
        """
        for _ in range(max_new_tokens):
            # Crop idx to the last BLOCK_SIZE tokens (model's context window).
            # This is crucial for long generations to keep context within model's limits.
            idx_cond = idx[:, -BLOCK_SIZE:]
            # Get predictions (logits) for the next token.
            logits, loss = self(idx_cond)
            # Focus only on the last time step (the predicted next token).
            logits = logits[:, -1, :] # (B, VOCAB_SIZE)

            # Apply repetition penalty
            if repetition_penalty != 1.0:
                for i in range(idx.shape[0]): # Iterate over batch
                    for prev_token_idx in set(idx[i].tolist()): # Get unique previous tokens
                        logits[i, prev_token_idx] /= repetition_penalty

            logits = logits / temperature # Apply temperature
            
            # Optionally apply top-k filtering
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')

            # Optionally apply top-p (nucleus) filtering
            if top_p is not None:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

                # Remove tokens with cumulative probability above the threshold
                sorted_indices_to_remove = cumulative_probs > top_p
                # Shift the indices to the right to keep at least one token above the threshold
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0

                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                logits[:, indices_to_remove] = -float('Inf')

            # Apply softmax to get probabilities.
            probs = F.softmax(logits, dim=-1)
            # Sample from the distribution to get the next token ID.
            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)
            # Append the sampled token to the sequence.
            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
        return idx
```

### 4.5.1 `__init__` (Initialization)

*   **`vocab_size`**: The total number of unique tokens the model can understand and generate. This comes from our tokenizer.
*   **`self.token_embedding_table = nn.Embedding(vocab_size, D_MODEL)`**:
    *   **Token Embeddings:** This layer converts each numerical token ID into a dense vector of size `D_MODEL`. These vectors capture the semantic meaning of words. For example, "king" and "queen" might have similar embeddings in certain dimensions.
*   **`self.position_embedding_table = nn.Embedding(BLOCK_SIZE, D_MODEL)`**:
    *   **Positional Embeddings:** Since Transformers don't inherently understand word order, we add positional information. This layer creates unique embeddings for each position up to `BLOCK_SIZE`.
*   **`self.blocks = nn.Sequential(...)`**: This creates a stack of `NUM_LAYERS` (e.g., 4) `TransformerBlock` instances. The `nn.Sequential` container ensures that the output of one block feeds directly into the next.
*   **`self.ln_f = nn.LayerNorm(D_MODEL)`**: A final Layer Normalization applied after all Transformer blocks.
*   **`self.lm_head = nn.Linear(D_MODEL, vocab_size)`**:
    *   **Language Model Head:** This is the final linear layer. It takes the output of the Transformer blocks (which is still in `D_MODEL` dimension) and projects it back to the `vocab_size`. The output of this layer are called **logits**, which represent the raw prediction scores for each possible next token in the vocabulary.
*   **`self.apply(self._init_weights)`**: This calls a helper function to initialize the weights of all linear and embedding layers. Good weight initialization is important for stable training.

### 4.5.2 `forward` (Training and Loss Calculation)

This method defines the forward pass of the model, used during both training and inference.

1.  **`tok_emb = self.token_embedding_table(idx)`**: Looks up the embedding vector for each token ID in the input `idx`.
2.  **`pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE))`**: Creates positional embeddings for the current sequence length `T`. `torch.arange(T)` generates numbers from 0 to `T-1`, representing positions.
3.  **`x = tok_emb + pos_emb`**: The token embeddings and positional embeddings are simply added together. This combines the semantic meaning of the token with its position in the sequence.
4.  **`x = self.blocks(x)`**: The combined embeddings are passed through the stack of Transformer blocks. Each block refines the representation by applying attention and feed-forward networks.
5.  **`x = self.ln_f(x)`**: Applies the final layer normalization.
6.  **`logits = self.lm_head(x)`**: The output of the Transformer blocks is projected to the vocabulary size to get the raw prediction scores (logits) for each possible next token.
7.  **`loss = F.cross_entropy(logits, targets)`**:
    *   **Cross-Entropy Loss:** If `targets` (the actual next tokens) are provided (which they are during training), the model calculates the **cross-entropy loss**. This loss function measures how well the model's predicted probability distribution over the vocabulary matches the true distribution (i.e., the actual next token). The goal during training is to minimize this loss.
    *   **Reshaping:** The `logits` and `targets` are reshaped to fit the expected input format of `F.cross_entropy`.

### 4.5.3 `generate` (Text Generation / Inference)

This method is used after the model is trained to generate new text.

*   **`@torch.no_grad()`**: Again, this decorator ensures no gradients are calculated, as we are only performing inference, not training.
*   **`for _ in range(max_new_tokens):`**: The model generates tokens one by one, up to `max_new_tokens`.
*   **`idx_cond = idx[:, -BLOCK_SIZE:]`**: This is a critical step for long generations. The model's context window is limited by `BLOCK_SIZE`. So, for each new token prediction, we only feed the *last* `BLOCK_SIZE` tokens of the current sequence into the model. This prevents the input sequence from growing indefinitely and exceeding memory limits.
*   **`logits, loss = self(idx_cond)`**: Performs a forward pass using only the conditioned (cropped) input. We only care about the `logits` here.
*   **`logits = logits[:, -1, :] / temperature`**:
    *   **`logits[:, -1, :]`**: We only need the logits for the *last* token in the input sequence, as that's the position for which we want to predict the *next* token.
    *   **` / temperature`**: **Temperature** is a hyperparameter used during text generation to control the randomness of the output.
        *   **High temperature (e.g., > 1.0):** Makes the probability distribution flatter, increasing the likelihood of sampling less probable (and thus more "creative" or "random") tokens.
        *   **Low temperature (e.g., < 1.0):** Makes the probability distribution sharper, increasing the likelihood of sampling the most probable tokens, leading to more deterministic and conservative output.
*   **`if top_k is not None:`**: **Top-k sampling** is another technique to control randomness. Instead of considering all possible next tokens, it only considers the `top_k` most probable tokens and redistributes their probabilities. This prevents the model from generating very unlikely (and often nonsensical) tokens.
*   **`probs = F.softmax(logits, dim=-1)`**: Converts the logits into a probability distribution over the vocabulary.
*   **`idx_next = torch.multinomial(probs, num_samples=1)`**: This is where the next token is actually chosen. `torch.multinomial` samples a token from the probability distribution. This introduces an element of randomness, making the generated text less repetitive.
*   **`idx = torch.cat((idx, idx_next), dim=1)`**: The newly sampled token (`idx_next`) is appended to the current sequence (`idx`), and the loop continues to predict the next token.

## 4.6 Conclusion

`src/model.py` brings together the theoretical concepts of Transformers into a working, trainable language model. By understanding how `MultiHeadSelfAttention`, `FeedForward`, and `TransformerBlock` interact, and how embeddings and the LM head function, you've gained a deep insight into the core architecture of modern LLMs. This foundation is crucial for understanding how these models learn to process and generate human-like text.
