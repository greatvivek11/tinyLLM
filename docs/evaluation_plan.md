### **Evaluation Plan for the TinyStories LLM**

**1. Objective**

The primary goal is to establish a robust and comprehensive evaluation framework for our `TinyLLM` model. This framework will allow us to:
*   Objectively measure the model's predictive performance.
*   Qualitatively assess the quality of its generated text.
*   Automate the qualitative assessment to ensure scalability and reduce human bias.

**2. Evaluation Components**

We will implement a three-pronged evaluation strategy:

**2.1. Quantitative Evaluation: Perplexity**

*   **Metric:** Perplexity measures how well a probability model predicts a sample. In language modeling, it indicates the model's uncertainty or "surprise" when predicting the next token in a sequence.
*   **Interpretation:** A lower perplexity score is better, signifying that the model's predictions are closer to the actual text.
*   **Implementation:** We will calculate the perplexity score on the held-out validation dataset (`val_data`). This ensures we are measuring the model's ability to generalize to unseen data.

**2.2. Qualitative Evaluation: Sample Generation**

*   **Metric:** Manual, human assessment of generated text.
*   **Criteria for Assessment:**
    *   **Coherence:** Does the story flow logically?
    *   **Fluency & Grammar:** Is the text grammatically correct and natural-sounding?
    *   **Creativity:** Does the model generate interesting and non-repetitive continuations from a prompt?
    *   **Relevance:** Does the generated text stay on topic with the initial prompt?
*   **Implementation:** The evaluation script will generate 3-5 sample stories from a predefined list of diverse prompts. These will be printed to the console for review.

**2.3. Automated Qualitative Evaluation: LLM-as-a-Judge**

*   **Metric:** An automated quality score generated by a powerful, external Large Language Model (e.g., GPT-4, Claude 3).
*   **Process:**
    1.  Our evaluation script will generate a story.
    2.  This story will be sent to the external "judge" LLM via an API call.
    3.  The API call will include a carefully crafted prompt instructing the judge to score the story on a scale of 1-10 across our defined criteria (Coherence, Creativity, Grammar).
    4.  The script will parse the judge's response (which will be in a structured format like JSON) to extract the scores.
*   **Implementation:**
    *   Requires adding a new dependency (e.g., `openai` or `anthropic` Python library).
    *   Requires an API key, which must be securely managed (e.g., via environment variables).
    *   The script will average the scores from the judge to produce a final, automated quality rating.

**3. Implementation Plan**

A new script, `src/main_eval.py`, will be created to orchestrate this entire process. It will contain functions for each component:
*   `load_model_and_tokenizer()`: Loads the saved `tinystories_llm_v1.pth` and the tokenizer.
*   `calculate_perplexity()`: Implements the perplexity calculation.
*   `generate_samples()`: Generates and prints stories for manual review.
*   `evaluate_with_llm_judge()`: Manages the API call to the external LLM and parses the results.
*   `main()`: The main function that calls the above functions in order and presents a final, consolidated report.
